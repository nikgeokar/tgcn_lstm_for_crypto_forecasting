{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77d95bb9",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>Dynamic Graph Convolution Neural Networks </center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532cc0d",
   "metadata": {},
   "source": [
    "## Generals "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a65109f",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Packages import and system configurations. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5dfe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import shutil\n",
    "from numpy import *\n",
    "#Graph Counstruction\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from torch_geometric_temporal.signal import StaticGraphTemporalSignal,DynamicGraphTemporalSignal\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric_temporal.nn.recurrent import A3TGCN2,TGCN2,DCRNN,MPNNLSTM,A3TGCN,GCLSTM,TGCN\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0736f0",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Define necessary paths. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1102ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_intermediate_path ='io/input/data_intermediate/'\n",
    "\n",
    "train_node_features_path = data_intermediate_path + 'node_features/train/'\n",
    "train_node_labels_path = data_intermediate_path + 'node_labels/train/'\n",
    "train_edges_path = data_intermediate_path + 'edges/train/'\n",
    "train_edge_weights_path = data_intermediate_path + 'edge_weights/train/'\n",
    "\n",
    "val_node_labels_path = data_intermediate_path + 'node_labels/val/'\n",
    "val_node_features_path = data_intermediate_path + 'node_features/val/'\n",
    "val_edges_path = data_intermediate_path + 'edges/val/'\n",
    "val_edge_weights_path = data_intermediate_path + 'edge_weights/val/'\n",
    "\n",
    "test_node_labels_path = data_intermediate_path + 'node_labels/test/'\n",
    "test_node_features_path = data_intermediate_path + 'node_features/test/'\n",
    "test_edges_path = data_intermediate_path + 'edges/test/'\n",
    "test_edge_weights_path = data_intermediate_path + 'edge_weights/test/'\n",
    "\n",
    "chunk_size = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6f2ea",
   "metadata": {},
   "source": [
    "## Core Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eda261",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Load numpy arrays on chunks. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405446ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_object(path):\n",
    "    num_chunks = len([f for f in os.listdir(path) if f.startswith('chunk_') and f.endswith('.npy')])\n",
    "    # Load array from chunks\n",
    "    chunks = []\n",
    "    for i in range(num_chunks):\n",
    "        filename = f\"{path}chunk_{i}.npy\"\n",
    "        chunk = np.load(filename,allow_pickle=True)\n",
    "        chunks.append(chunk)\n",
    "    arr_reconstructed = np.concatenate(chunks, axis=0)\n",
    "    return arr_reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd387d",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Load all graph information and print shapes\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91b5de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_node_features = load_object(train_node_features_path)\n",
    "train_node_labels = load_object(train_node_labels_path)\n",
    "train_edges = load_object(train_edges_path)\n",
    "train_edge_weights = load_object(train_edge_weights_path)\n",
    "\n",
    "val_node_features = load_object(val_node_features_path)\n",
    "val_node_labels = load_object(val_node_labels_path)\n",
    "val_edges = load_object(val_edges_path) ##\n",
    "val_edge_weights = load_object(val_edge_weights_path) ##\n",
    "\n",
    "test_node_features = load_object(test_node_features_path)\n",
    "test_node_labels = load_object(test_node_labels_path)\n",
    "test_edges = load_object(test_edges_path)\n",
    "test_edge_weights = load_object(test_edge_weights_path)\n",
    "\n",
    "print('Train Node Features Train',train_node_features.shape)\n",
    "print('Train Node Labels Train',train_node_labels.shape)\n",
    "print('Train Edges Shape',train_edges.shape)\n",
    "print('Train Edges Weights Shapes',train_edge_weights.shape)\n",
    "print('\\n')\n",
    "print('Validation Node Features Train',val_node_features.shape)\n",
    "print('Validation Node Labels Train',val_node_labels.shape)\n",
    "print('Validation Edges Shape',val_edges.shape)\n",
    "print('Validation Edges Weights Shapes',val_edge_weights.shape)\n",
    "print('\\n')\n",
    "print('Test Node Features Train',test_node_features.shape)\n",
    "print('Test Node Labels Train',test_node_labels.shape)\n",
    "print('Test Edges Shape',test_edges.shape)\n",
    "print('Test Edges Weights Shapes',test_edge_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaac64b",
   "metadata": {},
   "source": [
    "## Convert Graph info to loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2df286",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Zero edge weights according thresholds and exclude self edges\n",
    "<br>\n",
    "<br>\n",
    "This is used to optimize the edges according to our problem\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe58d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_threshold = 0.3\n",
    "\n",
    "train_edge_weights[train_edge_weights == 1] = 0 #We exclude self edges\n",
    "train_edge_weights[train_edge_weights < edge_threshold] = 0\n",
    "train_edge_weights = train_edge_weights * train_edge_weights\n",
    "\n",
    "val_edge_weights[val_edge_weights == 1] = 0 #We exclude self edges\n",
    "val_edge_weights[val_edge_weights < edge_threshold] = 0\n",
    "val_edge_weights = val_edge_weights * val_edge_weights\n",
    "\n",
    "test_edge_weights[test_edge_weights == 1] = 0 #We exclude self edges\n",
    "test_edge_weights[test_edge_weights < edge_threshold] = 0\n",
    "test_edge_weights = test_edge_weights * test_edge_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae533fb",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This is a costum loader that we use to have at the same loader, features, targets edges and edge-weigths\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd5244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_data_loader(graph,batch_size):\n",
    "    features = np.array(graph.features)\n",
    "    targets = np.array(graph.targets)\n",
    "    edges = np.array(graph.edge_indices)\n",
    "    edge_attr = np.array(graph.edge_weights)\n",
    "    features_tensor = torch.from_numpy(features).type(torch.FloatTensor)\n",
    "    targets_tensor = torch.from_numpy(targets).type(torch.FloatTensor)\n",
    "    edges_tensor = torch.from_numpy(edges).type(torch.LongTensor)\n",
    "    edge_attr_tensor = torch.from_numpy(edge_attr).type(torch.FloatTensor)\n",
    "    dataset_new = torch.utils.data.TensorDataset(features_tensor, edges_tensor, edge_attr_tensor, targets_tensor)\n",
    "    \n",
    "    class CustomDataLoader(torch.utils.data.DataLoader):\n",
    "        def __init__(self, dataset, batch_size, drop_last):\n",
    "            super().__init__(dataset, batch_size=batch_size, drop_last=drop_last)\n",
    "\n",
    "        def collate_fn(self, data):\n",
    "            features, edges, edge_attr, targets = zip(*data)\n",
    "            batch_features = torch.stack(features)\n",
    "            batch_edges = torch.stack(edges)\n",
    "            batch_edge_attr = torch.stack(edge_attr)\n",
    "            batch_targets = torch.stack(targets)\n",
    "            return batch_features, batch_edges, batch_edge_attr, batch_targets\n",
    "    graph_loader = CustomDataLoader(dataset_new, batch_size=batch_size, drop_last=False)\n",
    "    return graph_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d4988",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "We use this function to apply our loader\n",
    "</font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c793b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_data_loader(batch_size,train_edges,train_edge_weights,train_node_features,train_node_labels,\n",
    "                      test_edges,test_edge_weights,test_node_features,test_node_labels):\n",
    "\n",
    "    graph_train = DynamicGraphTemporalSignal(edge_indices=train_edges,edge_weights=train_edge_weights,\n",
    "                                   features=train_node_features,targets=train_node_labels)\n",
    "\n",
    "\n",
    "\n",
    "    graph_test = DynamicGraphTemporalSignal(edge_indices=test_edges,edge_weights=test_edge_weights,\n",
    "                                   features=test_node_features,targets=test_node_labels)\n",
    "\n",
    "\n",
    "    train_loader = dynamic_data_loader(graph_train,batch_size)\n",
    "    test_loader = dynamic_data_loader(graph_test,batch_size)\n",
    "    return train_loader,test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b7353a",
   "metadata": {},
   "source": [
    "## Graph Neural Networks Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02da1ae2",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Graph neural networks architecture \n",
    "</font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e40c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalGNN(torch.nn.Module):\n",
    "    def __init__(self, node_features, periods, batch_size):\n",
    "        super(TemporalGNN, self).__init__()\n",
    "        \n",
    "        self.tgnn = A3TGCN2(in_channels=node_features, periods=periods, out_channels=64, batch_size=batch_size)\n",
    "        \n",
    "        self.hidden2 = Linear(64, 32)        \n",
    "#         kaiming_uniform_(self.hidden2.weight, nonlinearity='relu') bidirectional=True\n",
    "        self.act2 = ReLU()\n",
    "           \n",
    "        self.hidden3 = Linear(32, 16)\n",
    "        self.act3 = ReLU()        \n",
    "\n",
    "        self.linear = torch.nn.Linear(16, periods)       \n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        h = self.tgnn(x, edge_index, edge_weight)\n",
    "        h = F.relu(h)\n",
    "\n",
    "        h = self.hidden2(h)\n",
    "        h = self.act2(h)\n",
    "        \n",
    "        h = self.hidden3(h)\n",
    "        h = self.act3(h)\n",
    "        \n",
    "        out = self.linear(h) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f45fb32",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Use the model to calculate metrics while predict using a test data loader\n",
    "</font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3650fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_evalaution_metrics(model,loader,loss_fn):\n",
    "    model.eval()\n",
    "    predictions=[]\n",
    "    ground_truth=[]\n",
    "\n",
    "    loss_list, mae_list ,r2_list, rmse_list = [], [], [], []\n",
    "    for snapsot in loader:\n",
    "        y_hat = model(snapsot[0], snapsot[1][-1], snapsot[2][-1])\n",
    "        y_true = y_hat\n",
    "        y_pred = snapsot[3]\n",
    "\n",
    "        loss = loss_fn(y_hat, snapsot[3])\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        mae = torch.mean(torch.abs(y_pred - y_true))\n",
    "        mae_list.append(mae.detach().numpy())\n",
    "        \n",
    "        r2 = torchmetrics.functional.r2_score(y_pred.view(-1), y_true.view(-1))\n",
    "        r2_list.append(r2.detach().numpy())\n",
    "\n",
    "        rmse = torch.sqrt(torch.mean(torch.pow(y_pred - y_true, 2)))\n",
    "        rmse_list.append(rmse.detach().numpy())\n",
    "        \n",
    "    loss = sum(loss_list) / len(loss_list)\n",
    "    mae = sum(mae_list) / len(mae_list)\n",
    "    r2 = sum(r2_list) / len(r2_list)\n",
    "    rmse = sum(rmse_list) / len(rmse_list)\n",
    "    return loss,mae,r2,rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e2c296",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a58ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(node_features,periods,batch_size,lr,epochs,train_loader,val_loader,loss_fn):\n",
    "    model = TemporalGNN(node_features=node_features, periods=periods, batch_size=batch_size)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    train_loss_ls,train_mae_ls,train_r2_ls,train_rmse_ls = [],[],[],[]\n",
    "    val_loss_ls,val_mae_ls,val_r2_ls,val_rmse_ls = [],[],[],[]\n",
    "    for epoch in range(epochs):\n",
    "        loss_list, mae_list ,r2_list, rmse_list = [], [], [], []\n",
    "        step = 0\n",
    "        for snapsot in tqdm(train_loader):\n",
    "            y_hat = model(snapsot[0], snapsot[1][0], snapsot[2][0])\n",
    "            loss = loss_fn(y_hat, snapsot[3])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            step= step+ 1\n",
    "            loss_list.append(loss.item())\n",
    "            # (!) ---> All the above code is for metrics calculation\n",
    "            y_true = y_hat\n",
    "            y_pred = snapsot[3]\n",
    "            mae = torch.mean(torch.abs(y_pred - y_true))\n",
    "            mae_list.append(mae.detach().numpy())\n",
    "            r2 = torchmetrics.functional.r2_score(y_pred.view(-1), y_true.view(-1))\n",
    "            r2_list.append(r2.detach().numpy())\n",
    "            rmse = torch.sqrt(torch.mean(torch.pow(y_pred - y_true, 2)))\n",
    "            rmse_list.append(rmse.detach().numpy())\n",
    "\n",
    "        train_loss = sum(loss_list) / len(loss_list)\n",
    "        train_mae = sum(mae_list) / len(mae_list)\n",
    "        train_r2 = sum(r2_list) / len(r2_list)\n",
    "        train_rmse = sum(rmse_list) / len(rmse_list)\n",
    "        val_loss,val_mae,val_r2,val_rmse = calculate_evalaution_metrics(model,val_loader,loss_fn)\n",
    "        print(\"Epoch {}, Train || MSE: {:.7f}, MAE: {:.7f}, R2: {:.7f}, RMSE: {:.7f}\".format(epoch+1,train_loss,train_mae,train_r2,train_rmse))\n",
    "        print(\"Epoch {}, Evaluation || MSE: {:.7f}, MAE: {:.7f}, R2: {:.7f}, RMSE: {:.7f}\".format(epoch+1,val_loss,val_mae,val_r2,val_rmse))\n",
    "        train_loss_ls.append(train_loss)\n",
    "        train_mae_ls.append(train_mae)\n",
    "        train_r2_ls.append(train_r2)\n",
    "        train_rmse_ls.append(train_rmse)\n",
    "        val_loss_ls.append(val_loss)\n",
    "        val_mae_ls.append(val_mae)\n",
    "        val_r2_ls.append(val_r2)\n",
    "        val_rmse_ls.append(val_rmse)\n",
    "    metrics = {'train_loss_ls': train_loss_ls,'train_mae_ls': train_mae_ls,'train_r2_ls': train_r2_ls,\n",
    "               'train_rmse_ls': train_rmse_ls,'eval_loss_ls': val_loss_ls,'eval_mae_ls': val_mae_ls,\n",
    "               'eval_r2_ls': val_r2_ls,'eval_rmse_ls': val_rmse_ls}\n",
    "    return model,metrics   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1fb391",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5843bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_metric(metric,metric_label,set_label):\n",
    "    smooth_metric = smooth_curve(metric)\n",
    "    plt.plot(range(1, len(smooth_metric) + 1), smooth_metric,label=set_label)\n",
    "    plt.title('Loss during Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric_label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b42d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_metrics_train_val(metrics,evaluation_mode):\n",
    "    smooth_mse_train = smooth_curve(metrics['train_loss_ls'])\n",
    "    smooth_mse_val = smooth_curve(metrics['eval_loss_ls'])\n",
    "    smooth_mae_train = smooth_curve(metrics['train_mae_ls'])\n",
    "    smooth_mae_val = smooth_curve(metrics['eval_mae_ls'])\n",
    "    smooth_r2_train = smooth_curve(metrics['train_r2_ls'])\n",
    "    smooth_r2_val = smooth_curve(metrics['eval_r2_ls'])\n",
    "    smooth_rmse_train = smooth_curve(metrics['train_rmse_ls'])\n",
    "    smooth_rmse_val = smooth_curve(metrics['eval_rmse_ls'])\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2,figsize=(12, 8))\n",
    "    # Plot the first metric on the top-left subplot\n",
    "    axs[0, 0].plot(range(1, len(smooth_mse_train) + 1), smooth_mse_train,label='Train')\n",
    "    axs[0, 0].plot(range(1, len(smooth_mse_val) + 1), smooth_mse_val,label=evaluation_mode)\n",
    "    axs[0, 0].set_xlabel('Epochs')\n",
    "    axs[0, 0].set_ylabel('MSE')\n",
    "    axs[0, 0].set_title('MSE')\n",
    "    axs[0, 0].legend()\n",
    "    # Plot the second metric on the top-right subplot\n",
    "    axs[0, 1].plot(range(1, len(smooth_mae_train) + 1), smooth_mae_train,label='Train')\n",
    "    axs[0, 1].plot(range(1, len(smooth_mae_val) + 1), smooth_mae_val,label=evaluation_mode)\n",
    "    axs[0, 1].set_xlabel('Epochs')\n",
    "    axs[0, 1].set_ylabel('MAE')\n",
    "    axs[0, 1].set_title('MAE')\n",
    "    axs[0, 1].legend()\n",
    "    # Plot the third metric on the bottom-left subplot\n",
    "    axs[1, 0].plot(range(1, len(smooth_r2_train) + 1), smooth_r2_train,label='Train')\n",
    "    axs[1, 0].plot(range(1, len(smooth_r2_val) + 1), smooth_r2_val,label=evaluation_mode)\n",
    "    axs[1, 0].set_xlabel('Epochs')\n",
    "    axs[1, 0].set_ylabel('R2')\n",
    "    axs[1, 0].set_title('R2')\n",
    "    axs[1, 0].legend()\n",
    "    # Plot the fourth metric on the bottom-right subplot\n",
    "    axs[1, 1].plot(range(1, len(smooth_rmse_train) + 1), smooth_rmse_train,label='Train')\n",
    "    axs[1, 1].plot(range(1, len(smooth_rmse_val) + 1), smooth_rmse_val,label=evaluation_mode)\n",
    "    axs[1, 1].set_xlabel('Epochs')\n",
    "    axs[1, 1].set_ylabel('RMSE')\n",
    "    axs[1, 1].set_title('RMSE')\n",
    "    axs[1, 1].legend()\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f319fd1",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a16c41",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization using Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0523f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "recurent_steps = batch_size\n",
    "node_features = train_node_features.shape[2]\n",
    "periods = 1\n",
    "lr = 0.001\n",
    "epochs = 2\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "train_loader,val_loader = graph_data_loader(batch_size,train_edges,train_edge_weights,train_node_features,train_node_labels,\n",
    "                                             val_edges,val_edge_weights,val_node_features,val_node_labels)\n",
    "\n",
    "model,metrics_val = model_training(node_features,periods,recurent_steps,lr,epochs,train_loader,val_loader,loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_metrics_train_val(metrics_val,'Validation')\n",
    "plot_single_metric(metrics_val['eval_mae_ls'],'MAE','Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb16a0ad",
   "metadata": {},
   "source": [
    "### Model's Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c75023",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "recurent_steps = batch_size/2\n",
    "node_features = train_node_features.shape[2]\n",
    "periods = 1\n",
    "lr = 0.001\n",
    "epochs = 2\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "train_edges_f = np.concatenate((train_edges, val_edges), axis=0)\n",
    "train_edge_weights_f = np.concatenate((train_edge_weights, val_edge_weights), axis=0)\n",
    "train_node_features_f = np.concatenate((train_node_features, val_node_features), axis=0)\n",
    "train_node_labels_f = np.concatenate((train_node_labels, val_node_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,test_loader = graph_data_loader(batch_size,train_edges_f,train_edge_weights_f,train_node_features_f,\n",
    "                                train_node_labels_f,test_edges,test_edge_weights,test_node_features,test_node_labels)\n",
    "\n",
    "model,metrics_test = model_training(node_features,periods,recurent_steps,lr,epochs,train_loader,test_loader,loss_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb2339",
   "metadata": {},
   "source": [
    "### Evaluation on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f18a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse,mae,r2,rmse = calculate_evalaution_metrics(model,test_loader,loss_fn)\n",
    "print(\"Evaluation on Test || MSE: {:.7f}, MAE: {:.7f}, R2: {:.7f}, RMSE: {:.7f}\".format(mse,mae,r2,rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55278013",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_metrics_train_val(metrics_test,'Test')\n",
    "plot_single_metric(metrics_test['val_mae_ls'],'MAE','Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0f0952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "full_ml",
   "language": "python",
   "name": "full_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
