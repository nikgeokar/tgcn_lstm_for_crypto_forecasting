{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db733ee1",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>Comparing the Performance of popular ML algorithms</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc6378",
   "metadata": {},
   "source": [
    "<center><font size=\"3\">\n",
    "In this notebook we structure several of the common machine learning algorithms to compare their results with our approach.\n",
    "<br>\n",
    "<br>\n",
    "Αiming to make as fair a comparison as possible, we perform hyper-parameter optimization οn every algorithm using the validation set. \n",
    "<br>\n",
    "<br>\n",
    "Then knowing the best combination of hyper-parameters we train each model and evaluate the model on test set by measuring various metrics\n",
    "\n",
    "</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a84bcd",
   "metadata": {},
   "source": [
    "## Generals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e92272",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Packages import and system configurations. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab887616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import shutil\n",
    "import math\n",
    "from datetime import datetime\n",
    "from datetime import datetime as dt\n",
    "import itertools\n",
    "#Graph Counstruction\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor \n",
    "from sklearn.neighbors import KNeighborsRegressor \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55210b91",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Define necessary paths. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056cf21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'io/input/base_data/train.csv'\n",
    "test_path = 'io/input/base_data/test.csv'\n",
    "val_path ='io/input/base_data/valid.csv'\n",
    "plot_export_path = 'io/output/exports/analysis_plots/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4c99b",
   "metadata": {},
   "source": [
    "## Core Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd3ae0",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Load data according to validation_ind (if true: return train-validation | else: return train - test) \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99904a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path,val_path,test_path,validation_ind):\n",
    "    if validation_ind:\n",
    "        trainset = pd.read_csv(train_path,index_col=0)\n",
    "        x_train,y_train = init_process(trainset) \n",
    "        valset = pd.read_csv(val_path,index_col=0)\n",
    "        x_test,y_test = init_process(valset)\n",
    "        x_train,x_test = scaller(x_train,x_test)\n",
    "    else:\n",
    "        trainset = pd.read_csv(train_path,index_col=0)\n",
    "        valset = pd.read_csv(val_path,index_col=0)\n",
    "        trainset = pd.concat([trainset, valset], ignore_index=True)\n",
    "        x_train,y_train = init_process(trainset) \n",
    "        testset = pd.read_csv(test_path,index_col=0)\n",
    "        x_test,y_test = init_process(testset)\n",
    "        x_train,x_test = scaller(x_train,x_test)\n",
    "    return x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f282e04f",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Split: features -  target\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_process(df):\n",
    "    y = df['Target']\n",
    "    x = df.drop(['Target'], axis=1)\n",
    "    x = x.drop(['Asset_id'], axis=1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9de3a6",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Apply Standardization. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13431eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaller(x_train,x_test):\n",
    "    Standar_Scaller = StandardScaler()\n",
    "    scalled_train_data= Standar_Scaller.fit_transform(x_train)\n",
    "    scalled_test_data = Standar_Scaller.transform(x_test)\n",
    "    return scalled_train_data,scalled_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae6d7f",
   "metadata": {},
   "source": [
    "## Hyper-Parameters Tunig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae83664",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "A core function that apply the evaluation procces using train & validation set:\n",
    "<ol>\n",
    "<li>Initialize the given regression models (SVR, DecisionTreeRegressor, XGBRegressor & more) with default hyperparameters.</li>\n",
    "<li>Define the hyperparameter search spaces (ranges of values to try for each hyperparameter) for each model.</li>\n",
    "<li>Loop through the three models and their corresponding hyperparameter search spaces.</li>\n",
    "<li>Generate all possible combinations of hyperparameters for each model.</li>\n",
    "<li>For each hyperparameter combination, fit the model on the training set, make predictions on the validation set, and calculate the MAE score.</li>\n",
    "<li>Track the best hyperparameters for each model based on the lowest MAE score on the validation set.</li>\n",
    "<li>Return the best hyperparameters for each model as a list of dictionaries.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_tuning_validation(x_train, y_train, x_val, y_val):\n",
    "    # Initialize the models\n",
    "    svm_reg = SVR()\n",
    "    tree_reg = DecisionTreeRegressor()\n",
    "    xgb_reg = XGBRegressor()\n",
    "    nb_reg = GaussianProcessRegressor()\n",
    "    lr_reg = LinearRegression()\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    rf_reg = RandomForestRegressor()\n",
    "    gb_reg = GradientBoostingRegressor()\n",
    "    ada_reg = AdaBoostRegressor()\n",
    "\n",
    "    # Define the hyperparameter ranges\n",
    "    svm_param_grid = {'kernel': ['linear','sigmoid', 'rbf'],'C': [0.1, 10, 15],'gamma':['scale','auto']}\n",
    "    tree_param_grid = {'criterion':['squared_error','friedman_mse'],'max_depth': [6,10,12], 'min_samples_split': [4, 6, 8]}\n",
    "    xgb_param_grid = {'n_estimators': [6, 10, 25], 'max_depth': [1, 2, 5], 'eta':[0.1,0.3,0.5]}\n",
    "    nb_param_grid = {'kernel': [1**2 * RBF(length_scale=1)],'alpha': [0.1, 1.0, 10.0],'n_restarts_optimizer': [0, 1, 2],'optimizer': ['fmin_l_bfgs_b']}\n",
    "    lr_param_grid = {'fit_intercept': [True, False], 'normalize': [True, False]}\n",
    "    knn_param_grid = {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance'], 'algorithm': ['ball_tree', 'kd_tree', 'brute']}\n",
    "    rf_param_grid = {'max_depth': [3, 5, 7], 'n_estimators': [50, 100, 200], 'min_samples_split': [2, 4, 6]}\n",
    "    gb_param_grid = {'learning_rate': [0.01, 0.1, 1.0], 'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7]}\n",
    "    ada_param_grid = {'learning_rate': [0.01, 0.1, 1.0], 'n_estimators': [50, 100, 200]}\n",
    "\n",
    "    \n",
    "    models = [svm_reg, tree_reg, xgb_reg, nb_reg, lr_reg, knn_reg, rf_reg, gb_reg, ada_reg]\n",
    "    param_grids = [svm_param_grid, tree_param_grid, xgb_param_grid, nb_param_grid, lr_param_grid,\n",
    "                  knn_param_grid, rf_param_grid, gb_param_grid, ada_param_grid]\n",
    "\n",
    "    best_params = []\n",
    "    for i, model in enumerate(models):\n",
    "        print('\\n')\n",
    "        best_mae = float('inf')\n",
    "        best_mape = float('inf')\n",
    "        best_params_i = {}\n",
    "        # Generate all possible combinations of hyperparameters\n",
    "        param_combinations = list(itertools.product(*(param_grids[i][param] for param in param_grids[i])))\n",
    "        for j, params in enumerate(param_combinations):\n",
    "            # Unpack the tuple of parameter values into individual arguments\n",
    "            params_dict = dict(zip(param_grids[i], params))\n",
    "            model.set_params(**params_dict)\n",
    "            model.fit(x_train, y_train)\n",
    "            y_val_pred = model.predict(x_val)\n",
    "            mse = round(mean_squared_error(y_val, y_val_pred), 7)\n",
    "            mae = round(mean_absolute_error(y_val, y_val_pred),7)\n",
    "            r2 = round(r2_score(y_val, y_val_pred),7)\n",
    "            rmse = round(np.sqrt(mean_squared_error(y_val, y_val_pred)), 7)\n",
    "            param_str = ', '.join([f'{param}={value}' for param, value in params_dict.items()])\n",
    "            print(f\"Experiment {j+1} with {model.__class__.__name__} using {param_str} has MSE: {mse}, MAE: {mae}, R2: {r2}, RMSE: {rmse}\")\n",
    "            if mae < best_mae:\n",
    "                best_mae = mae\n",
    "                best_r2 = r2\n",
    "                best_params_i = dict(zip(param_grids[i], params))\n",
    "        best_params.append(best_params_i)\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f853f6a9",
   "metadata": {},
   "source": [
    "## Model Train & Test Evalaution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7330354",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "A core function that apply the final train and evaluation procces using train & test set:\n",
    "<ol>\n",
    "<li>Initialize the models with the best hyperparameters.</li>\n",
    "<li>Fit each model on the training data.</li>\n",
    "<li>Use the fitted models to make predictions on the test set.</li>\n",
    "<li>Compute the evaluation metrics (MSE, MAE, R2, RMSE) for each model.</li>\n",
    "<li>Store the results for each model in a dictionary.</li>\n",
    "<li>Print the evaluation metrics for each model.</li>\n",
    "<li>Return the dictionary containing the results.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3397e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(x_train, y_train, x_test, y_test, best_params):\n",
    "    # Initialize the models with the best hyperparameters\n",
    "    svm_reg = SVR(**best_params[0])\n",
    "    tree_reg = DecisionTreeRegressor(**best_params[1])\n",
    "    xgb_reg = XGBRegressor(**best_params[2])\n",
    "    nb_reg = GaussianProcessRegressor(**best_params[3])\n",
    "    lr_reg = LinearRegression(**best_params[4])\n",
    "    knn_reg = KNeighborsRegressor(**best_params[5])\n",
    "    rf_reg = RandomForestRegressor(**best_params[6])\n",
    "    gb_reg = GradientBoostingRegressor(**best_params[7])\n",
    "    ada_reg = AdaBoostRegressor(**best_params[8])\n",
    "\n",
    "    models = [svm_reg, tree_reg, xgb_reg, nb_reg, lr_reg, knn_reg, rf_reg, gb_reg, ada_reg]\n",
    "    model_names = ['SVM', 'Decision Tree', 'XGBoost', 'Gaussian NB', 'Linear Regression',\n",
    "                  'KNN', 'Random Forest', 'Gradient Boosting','AdaBoost']\n",
    "    \n",
    "    results = {}\n",
    "    for i, model in enumerate(models):\n",
    "        model.fit(x_train, y_train)\n",
    "        y_test_pred = model.predict(x_test)\n",
    "        mse = round(mean_squared_error(y_test, y_test_pred), 7)\n",
    "        mae = round(mean_absolute_error(y_test, y_test_pred),7)\n",
    "        r2 = round(r2_score(y_test, y_test_pred),7)\n",
    "        rmse = round(np.sqrt(mean_squared_error(y_test, y_test_pred)), 7)\n",
    "        results[model_names[i]] = {'MSE': mse,'MAE': mae, 'R2': r2, 'RMSE': rmse}\n",
    "        print(f\"{model_names[i]} model has MSE: {mse}, MAE: {mae}, R2: {r2}, RMSE: {rmse}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6414bf4",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Plot metrics on subplots for camparison purposes\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_metrics_train_val(metrics):\n",
    "    model_names = list(results.keys())\n",
    "    mse = [results[model]['MSE'] for model in model_names]\n",
    "    mae = [results[model]['MAE'] for model in model_names]\n",
    "    r2 = [results[model]['R2'] for model in model_names]\n",
    "    rmse = [results[model]['RMSE'] for model in model_names]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2,figsize=(12, 8))\n",
    "    fig.suptitle('Comparison of Model Performance', fontsize=16)\n",
    "    # Plot the first metric on the top-left subplot\n",
    "    axs[0, 0].bar(x - width/2, mse, width, label='MSE')\n",
    "    axs[0, 0].set_ylabel('MSE')\n",
    "    axs[0, 0].set_title('MSE')\n",
    "    axs[0, 0].set_xticks(x)\n",
    "    axs[0, 0].set_xticklabels(model_names)\n",
    "    #axs[0, 0].set_ylim(0,1.2)\n",
    "    axs[0, 0].legend()\n",
    "    # Plot the second metric on the top-right subplot\n",
    "    axs[0, 1].bar(x - width/2, mae, width, label='MAE')\n",
    "    axs[0, 1].set_ylabel('MAE')\n",
    "    axs[0, 1].set_title('MAE')\n",
    "    axs[0, 1].set_xticks(x)\n",
    "    axs[0, 1].set_xticklabels(model_names)\n",
    "    #axs[0, 1].set_ylim(0,1.2)\n",
    "    axs[0, 1].legend()\n",
    "    # Plot the third metric on the bottom-left subplot\n",
    "    axs[1, 0].bar(x - width/2, r2, width, label='R2')\n",
    "    axs[1, 0].set_ylabel('R2')\n",
    "    axs[1, 0].set_title('R2')\n",
    "    axs[1, 0].set_xticks(x)\n",
    "    axs[1, 0].set_xticklabels(model_names)\n",
    "    #axs[1, 0].set_ylim(0,1.2)\n",
    "    axs[1, 0].legend()\n",
    "    # Plot the fourth metric on the bottom-right subplot\n",
    "    axs[1, 1].bar(x - width/2, rmse, width, label='RMSE')\n",
    "    axs[1, 1].set_ylabel('RMSE')\n",
    "    axs[1, 1].set_title('RMSE')\n",
    "    axs[1, 1].set_xticks(x)\n",
    "    axs[1, 1].set_xticklabels(model_names)\n",
    "    #axs[1, 1].set_ylim(0,1.2)\n",
    "    axs[1, 1].legend()\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d9cb5",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb762e",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Hyper-Parameter tuning\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592580af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_val,y_val = load_data(train_path,val_path,test_path,validation_ind=True)\n",
    "best_params = parameter_tuning_validation(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d562e",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Training and evaluation using testset\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params=[{'kernel': 'linear', 'C': 0.1, 'gamma': 'scale'},\n",
    "#  {'criterion': 'squared_error', 'max_depth': 6, 'min_samples_split': 4},\n",
    "#  {'n_estimators': 25, 'max_depth': 1, 'eta': 0.3}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b13f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,y_test = load_data(train_path,val_path,test_path,validation_ind=False)\n",
    "results =  evaluate_models(x_train, y_train, x_test, y_test, best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150be8c3",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Plot results for comparison purposes \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_metrics_train_val(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf3ced3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "full_ml",
   "language": "python",
   "name": "full_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
