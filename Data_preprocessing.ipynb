{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c67e1d0",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>Data Preprocessing </center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37b206e",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "This notebook is dedicated to data processing, whereby raw data is inputted and transformed into final dataframes. \n",
    "<br>\n",
    "<br>\n",
    "The process includes the following steps:\n",
    "<li>Loading the raw data files as exported from the Bihance API.</li>\n",
    "<li>Merging the necessary files to obtain a single dataframe that contains information for all cryptocurrencies.</li>\n",
    "<li>Conducting feature engineering to create the necessary features.</li>\n",
    "<li>Creating the target variable, which is the next close for each time step.</li>\n",
    "<li>Dividing the data into train, validation, and test sets.</li>\n",
    "<li>Saving the final dataframes for use in subsequent notebooks.</li>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2315fb",
   "metadata": {},
   "source": [
    "## Generals "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ee2b41",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Packages import and system configurations. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff29572e-e6b3-4f40-ab35-f7622f87b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d3c640-ba4e-4801-8df7-0981c3d3df53",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b76d49",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The aim of the bellow funciton is to load crypto coin data and concatenate with RSI, MACD, and MACD_RSI if specified.\n",
    "<br>\n",
    "<br>\n",
    "The function performs the following steps:\n",
    "<ol>\n",
    "<li>Check if RSI, MACD, or MACD_RSI is specified.</li>\n",
    "<li>If not specified, read the raw data of the coin.</li>\n",
    "<li>If RSI or MACD is specified, read the raw data and the RSI or MACD data for the coin, concatenate the two dataframes, and drop the 'Time' column.</li>\n",
    "<li>If MACD_RSI is specified, read the raw data, MACD data, and RSI data for the coin, concatenate the three dataframes, and drop the 'Time' column for MACD and RSI.</li>\n",
    "<li>Drop any rows with null values.</li>\n",
    "<li>Reset the index of the dataframe and drop the 'index' column.</li>\n",
    "<li>Print the first five rows of the dataframe.</li>\n",
    "<li>Return the concatenated dataframe.</li>\n",
    "</ol> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f886b079-e20d-4fb3-a2e0-19be9aba6854",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_coins = ['ADA', 'BNB', 'BTC', 'DASH', 'ETH', 'LINK', 'LTC', 'XRP']\n",
    "conc = None # \"rsi\", \"macd\", \"macd_rsi\"\n",
    "\n",
    "def read_coin_data(coin_name: str, conc) -> pd.DataFrame:\n",
    "    \n",
    "    if conc == None:\n",
    "        raw_data_path = join(\"io\", \"input\", \"data_raw\", \"Crypto_July_2019_2023\", \"4H_2019\", coin_name ,f\"{coin_name.lower()}_2019.csv\")\n",
    "        data_df = pd.read_csv(raw_data_path, index_col=False)\n",
    "        return data_df\n",
    "    \n",
    "    elif (conc == \"rsi\") or (conc == \"macd\"):\n",
    "        raw_data_path1 = join(\"io\", \"input\", \"data_raw\", \"Crypto_July_2019_2023\", \"4H_2019\", coin_name , f\"{coin_name.lower()}_2019.csv\")\n",
    "        raw_data_path2 = join(\"io\", \"input\", \"data_raw\", \"Crypto_July_2019_2023\", \"4H_2019\", coin_name, f\"{coin_name.lower()}_4h_{conc}.csv\")\n",
    "\n",
    "        data_df1 = pd.read_csv(raw_data_path1, index_col=False)\n",
    "        data_df2 = pd.read_csv(raw_data_path2, index_col=False)\n",
    "        data_df2 = data_df2.drop('Time', axis=1)\n",
    "        data_df2 = data_df2.rename(columns={\"0\": conc})\n",
    "        \n",
    "        concatenated = pd.concat([data_df1, data_df2], axis=\"columns\")\n",
    "\n",
    "        \n",
    "    elif conc == \"macd_rsi\":      \n",
    "        raw_data_path1 = join(\"io\", \"input\", \"data_raw\", \"Crypto_July_2019_2023\", \"4H_2019\", coin_name , f\"{coin_name.lower()}_2019.csv\")\n",
    "        raw_data_path2 = join(\"io\", \"input\", \"data_raw\", \"Crypto_July_2019_2023\", \"4H_2019\", coin_name, f\"{coin_name.lower()}_4h_macd.csv\")\n",
    "        raw_data_path3 = join(\"io\", \"input\", \"data_raw\", \"Crypto_July_2019_2023\", \"4H_2019\", coin_name, f\"{coin_name.lower()}_4h_rsi.csv\")\n",
    "        \n",
    "        data_df1 = pd.read_csv(raw_data_path1, index_col=False)\n",
    "        data_df2 = pd.read_csv(raw_data_path2, index_col=False)\n",
    "        data_df3 = pd.read_csv(raw_data_path3, index_col=False)\n",
    "        \n",
    "        data_df2 = data_df2.drop('Time', axis=1)\n",
    "        data_df2 = data_df2.rename(columns={\"0\": \"macd\"})\n",
    "        \n",
    "        data_df3 = data_df3.drop('Time', axis=1)\n",
    "        data_df3 = data_df3.rename(columns={\"0\": \"rsi\"})\n",
    "\n",
    "        concatenated = pd.concat([data_df1, data_df2, data_df3], axis=\"columns\")\n",
    "\n",
    "      \n",
    "        \n",
    "    concatenated = concatenated.dropna(axis=0)\n",
    "    concatenated = concatenated.reset_index()\n",
    "    concatenated = concatenated.drop('index', axis=1)\n",
    "    print(concatenated.head())\n",
    "\n",
    "    return concatenated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d6c9b6-44cf-4b8a-a32e-1e2e54e03f3e",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0cad7-1fe1-43cb-9f76-215071f6199f",
   "metadata": {},
   "source": [
    "### Time & date features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca877d",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The aim of the bellow funciton is to add date-related features to a DataFrame.\n",
    "<br>\n",
    "<br>\n",
    "The function performs the following steps:\n",
    "<ol>\n",
    "<li>Converts the 'Date' column to datetime format.</li>\n",
    "<li>Extracts and adds the year, month, day, week of year, and hour as new columns.</li>\n",
    "<li>Returns the updated DataFrame.</li>\n",
    "</ol> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35476a-eaa0-4593-8168-cd0bfe59f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_date_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "    df['Week_of_Year'] = df['Date'].dt.isocalendar().week\n",
    "    df['Hour'] = df['Date'].dt.hour\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186c911c",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The aim of the bellow funciton is to create trigonometric columns for Year, Month, Day, and Hour. It converts the values.\n",
    "<br>\n",
    "<br>\n",
    "The function performs the following steps:\n",
    "<ol>\n",
    "<li>Apply sine and cosine functions to Year, Month, Day, and Hour columns to create new columns.</li>\n",
    "<li>Drop the original columns for Year, Month, Day, Week_of_Year, and Hour from the DataFrame.</li>\n",
    "<li>Return the modified DataFrame.</li>\n",
    "</ol> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c280aa-eed9-48c4-891d-c6ea57620f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trigonometric_columns(df) -> pd.DataFrame:\n",
    "    # Create sine and cosine columns for Year, Month and Day\n",
    "    df['Year_sin'] = df['Year'].apply(lambda x: math.sin(2*math.pi*x/2023))\n",
    "    df['Year_cos'] = df['Year'].apply(lambda x: math.cos(2*math.pi*x/2023))\n",
    "    df['Month_sin'] = df['Month'].apply(lambda x: math.sin(2*math.pi*x/12))\n",
    "    df['Month_cos'] = df['Month'].apply(lambda x: math.cos(2*math.pi*x/12))\n",
    "    df['Day_sin'] = df['Day'].apply(lambda x: math.sin(2*math.pi*x/31))\n",
    "    df['Day_cos'] = df['Day'].apply(lambda x: math.cos(2*math.pi*x/31))\n",
    "    df['Hour_sin'] = df['Hour'].apply(lambda x: math.sin(2*math.pi*x/24))\n",
    "    df['Hour_cos'] = df['Hour'].apply(lambda x: math.cos(2*math.pi*x/24))\n",
    "    df = df.drop(['Year'], axis=1)\n",
    "    df = df.drop(['Month'], axis=1)\n",
    "    df = df.drop(['Day'], axis=1)\n",
    "    df = df.drop(['Week_of_Year'], axis=1)\n",
    "    df = df.drop(['Hour'], axis=1)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a26a94-da49-4905-a292-a7bb31845cca",
   "metadata": {},
   "source": [
    "### Create target column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849b38e1",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The aim of the bellow funciton is to create a target variable by shifting the \"Close\" column forward by a specified number \n",
    "<br>\n",
    "<br>\n",
    "The function performs the following steps:\n",
    "<ol>\n",
    "<li>Define the target column and create a list of feature columns.</li>\n",
    "<li>Create a new column for the shifted target variable and remove the last rows of the dataframe.</li>\n",
    "<li>Return the modified DataFrame.</li>\n",
    "</ol> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067929c3-594d-4b3f-a089-9281f421dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_variable(df: pd.DataFrame, forecast_lead: int = 1) -> pd.DataFrame:    \n",
    "    target_column = \"Close\"\n",
    "    features = list(df.columns.difference([target_column]))\n",
    "    \n",
    "    target = f\"{target_column}_lead_{forecast_lead}\"\n",
    "    df[target] = df[target_column].shift(-forecast_lead)\n",
    "    df = df.iloc[:-forecast_lead]\n",
    "    # display(\"Target added to dataframe\", df.shape, df[['Close', target]].head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8aa02",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The aim of the bellow funciton is to create a  consolidated dataframe containing historical data of various cryptocurrencies with additional features and target variables for time series forecasting.\n",
    "<br>\n",
    "<br>\n",
    "The function performs the following steps:\n",
    "<ol>\n",
    "<li>Create an empty dataframe with appropriate columns based on the concatenation parameter (if provided).</li>\n",
    "<li>Loop through each cryptocurrency in the list and retrieve its historical data.</li>\n",
    "<li>Process the data by appending date features, creating trigonometric columns, and generating a target variable.</li>\n",
    "<li>Concatenate the processed data with the consolidated dataframe.</li>\n",
    "<li>Perform one-hot encoding on the \"Asset_id\" column and drop unnecessary columns.</li>\n",
    "</ol> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc8192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coins_data(coins: list, conc) -> pd.DataFrame:\n",
    "    if conc == None:\n",
    "        df = pd.DataFrame(columns=[\"Open\", \"High\",\"Low\", \"Close\", \"Volume\"])\n",
    "    elif (conc == \"rsi\") or (conc == \"macd\"):\n",
    "        df = pd.DataFrame(columns=[\"Open\", \"High\",\"Low\", \"Close\", \"Volume\", conc])\n",
    "    elif conc == \"macd_rsi\":\n",
    "        df = pd.DataFrame(columns=[\"Open\", \"High\",\"Low\", \"Close\", \"Volume\", \"macd\", \"rsi\"])\n",
    "    \n",
    "    for coin in coins:\n",
    "        coin_df = read_coin_data(coin_name=coin, conc=conc)\n",
    "        coin_df[\"Asset_id\"] = coin\n",
    "        coin_df = coin_df.rename(columns={\"Time\":\"Date\"})\n",
    "        coin_df = append_date_features(df=coin_df)\n",
    "        coin_df = create_trigonometric_columns(df=coin_df)\n",
    "        coin_df = create_target_variable(df=coin_df, forecast_lead= 1)\n",
    "        # Set date as index\n",
    "        coin_df.set_index('Date', inplace=True)\n",
    "        df = pd.concat([df, coin_df])\n",
    "        \n",
    "        \n",
    "    one_hot = pd.get_dummies(df['Asset_id'])\n",
    "    df = pd.concat([df, one_hot], axis=1)\n",
    "    \n",
    "    df['Target'] = df['Close_lead_1']\n",
    "     \n",
    "    df = df.drop(['Close_lead_1'], axis=1)\n",
    "    display(\"Consolidated dataframe shape\",df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ac3ab-9beb-40f4-9aca-78a9f478faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_df = get_coins_data(coins=available_coins, conc=conc)\n",
    "consolidated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f4404-457a-494f-a901-83578ff88a2d",
   "metadata": {},
   "source": [
    "## Split dataset (Train, Validarion, Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa870b",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The aim of the bellow funciton is to split a given DataFrame into training, validation, and testing sets based on specific dates.\n",
    "<br>\n",
    "<br>\n",
    "The function performs the following steps:\n",
    "<ol>\n",
    "<li>Define two split dates for training, validation, and testing sets.</li>\n",
    "<li>Filter the DataFrame based on the split dates to create three separate sets: train, valid, and test.</li>\n",
    "<li>Print out the fractions and shapes of each set.</li>\n",
    "<li>Return the three sets as separate DataFrames.</li>\n",
    "</ol> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c93a99-55ea-4e58-8754-8f1565a7dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid_test(data: pd.DataFrame):    \n",
    "    # Split the data into training and testing sets\n",
    "    split_date_1 = datetime(2022, 1, 1)\n",
    "    split_date_2 = datetime(2022, 12, 1)\n",
    "    train_data = data.loc[data.index < split_date_1]\n",
    "    valid_data = data.loc[(split_date_1<= data.index) & (data.index <= split_date_2)]\n",
    "    test_data = data.loc[data.index > split_date_2]\n",
    "    \n",
    "    print(\"Train set fraction:\", round((len(train_data) / len(data)), 2),'%', \"- train shape -> \", train_data.shape)\n",
    "    print(\"Valid set fraction:\", round((len(valid_data) / len(data)), 2),'%', \"- valid shape -> \", valid_data.shape)\n",
    "    print(\"Test set fraction:\", round((len(test_data) / len(data)), 2),'%', \"- test shape -> \", test_data.shape)\n",
    "    return train_data, valid_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b1aa7-8498-468f-99da-9252d4cfe693",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = split_train_valid_test(data=consolidated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2451a0c9",
   "metadata": {},
   "source": [
    "### Save Final Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090fbc0d-dd7f-49ce-81dd-9df80edbf0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = join(\"io\", \"input\", \"base_data\")\n",
    "train_data.to_csv(join(output_path, \"train.csv\"))\n",
    "valid_data.to_csv(join(output_path, \"valid.csv\"))\n",
    "test_data.to_csv(join(output_path, \"test.csv\"))\n",
    "print(\"Datasets saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5051183",
   "metadata": {},
   "source": [
    "### Plot Time-Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadf1fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=consolidated_df\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# assume your dataframe is called df and has columns 'Asset_id' and 'Close'\n",
    "grouped = df.groupby('Asset_id')\n",
    "\n",
    "for asset_id, group in grouped:\n",
    "    # create a new figure for each plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # plot the timeseries for this asset_id\n",
    "    group.plot(y='Close', ax=ax)\n",
    "    \n",
    "    # set title and axis labels\n",
    "    ax.set_title(f\"Asset {asset_id} Close Timeseries\")\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Target')\n",
    "    \n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
