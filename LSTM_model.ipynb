{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12a1a8b",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Long Short-Term Memory (LSTM) </h1>\n",
    "\n",
    "Cryptocurrency has become a popular and volatile investment in recent years, and forecasting the prices of various coins has become an important task for traders and investors. In this Jupyter notebook, we will use Long Short-Term Memory (LSTM) models to predict the closing prices of eight different cryptocurrencies. We will use PyTorch to implement the LSTM models and evaluate the performance of each model with various metrics.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Data cleaning, feature engineering, and preprocessing: We will start by cleaning and preparing our data, which includes feature engineering and scaling the data.\n",
    "\n",
    "2. Train-validation-test split: We will split the data into training, validation, and test sets, where the validation set will be used for monitoring the training process of the LSTM models, and the test set will be used for evaluating the final predictions.\n",
    "\n",
    "3. PyTorch Dataset class initialization: We will create a PyTorch Dataset class object for our data to create the PyTorch dataloaders for our LSTM models.\n",
    "\n",
    "4. Multilayer LSTM model architecture initialization: We will initialize the architecture of our multilayer LSTM model.\n",
    "\n",
    "5. Training: We will train one LSTM model for each of the eight coins and evaluate the progress of training with metrics such as mean squared error (MSE), mean absolute error (MAE), coefficient of determination (R^2), and root mean squared error (RMSE) using the validation set.\n",
    "\n",
    "6. Evaluation: We will evaluate each model and then compute the mean prediction for the eight coins.\n",
    "\n",
    "7. Saving: We will save the test predictions and actual values and metrics for each coin to plot and evaluate later.\n",
    "\n",
    "By following these steps, we hope to generate accurate predictions for cryptocurrency prices that can be used for trading or investment decisions, while also identifying the best LSTM architectures and hyperparameters for this task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bec5ac",
   "metadata": {},
   "source": [
    "### Package imports and system configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61badde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from os.path import join\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf132073",
   "metadata": {},
   "source": [
    "### Necessary paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7bed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw_path ='io/input/data_raw/Crypto_July_2019_2023/4H_2019'\n",
    "export_path = 'io/output/exports/'\n",
    "test_path = 'io/input/base_data/test.csv'\n",
    "\n",
    "predictions_path = export_path + 'predictions/'\n",
    "metrics_plot_path = export_path + 'metrics_plots/'\n",
    "results_path = export_path + 'experiments_results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3adcce-a56c-4c2a-b54e-c66dd1fe98b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b209440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_coins = ['ADA', 'BNB', 'BTC', 'DASH', 'ETH', 'LINK', 'LTC', 'XRP']\n",
    "df = pd.DataFrame({'Available coins': available_coins})\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98524b74-fe35-4835-8418-c0ab2316a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_coin_data(coin_name: str) -> pd.DataFrame:    \n",
    "    data_df = pd.read_csv(f\"{data_raw_path}/{coin_name}/{coin_name.lower()}_2019.csv\",index_col=False)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af423c-43e6-491e-b374-8590c85c2e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_name = 'BTC'\n",
    "coin_df = read_coin_data(coin_name=coin_name)\n",
    "coin_df = coin_df.rename(columns={\"Time\":\"Date\"})\n",
    "coin_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaccd23-756c-4972-b70d-9778bcebf781",
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ddcb3b-1355-4084-a192-83fcd72b9967",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae184cd-d258-4b31-969c-214ca0e7af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coin_interactive(plot_df):\n",
    "    plot_df = coin_df.copy()\n",
    "    plot_df['Date'] = pd.to_datetime(plot_df['Date'])\n",
    "    fig = px.line(plot_df, x='Date', y='Close', title=f'{coin_name} Close Price Over Time')\n",
    "    fig.update_layout(xaxis_title='Date', yaxis_title='Close Price', xaxis_tickangle=-45)\n",
    "    fig.show()\n",
    "    \n",
    "def plot_coin_static(plot_df):\n",
    "    plot_df = coin_df.copy()\n",
    "    plot_df['Date'] = pd.to_datetime(plot_df['Date'])\n",
    "\n",
    "    plt.plot(plot_df['Date'], plot_df['Close'])\n",
    "    plt.title(f'{coin_name} Close Price Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.xticks(rotation=-45)\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd1202",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coin_interactive(plot_df=coin_df)\n",
    "plot_coin_static(plot_df=coin_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec972ce1-327c-4d0c-a3a9-18048a0e2972",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature Engineering\n",
    "* The purpose of the `append_date_features` function is to add additional date-related features to a pandas DataFrame. It enhances the original DataFrame by extracting various date components from a column named \"Date\" and appending them as separate columns.\n",
    "\n",
    "* The purpose of the `create_trigonometric_columns` function is to create trigonometric representations of date-related columns in a pandas DataFrame. By converting the date components into sine and cosine values, **it captures cyclical patterns in a continuous numerical form**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b36bcac-22d9-43b3-bdcc-38486cabac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_date_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "    df['Week_of_Year'] = df['Date'].dt.isocalendar().week\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59be1d7-496e-429e-9a14-931f46196d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trigonometric_columns(df) -> pd.DataFrame:\n",
    "    # Create sine and cosine columns for Year, Month and Day\n",
    "    df['Year_sin'] = df['Year'].apply(lambda x: math.sin(2*math.pi*x/2023))\n",
    "    df['Year_cos'] = df['Year'].apply(lambda x: math.cos(2*math.pi*x/2023))\n",
    "    df['Month_sin'] = df['Month'].apply(lambda x: math.sin(2*math.pi*x/12))\n",
    "    df['Month_cos'] = df['Month'].apply(lambda x: math.cos(2*math.pi*x/12))\n",
    "    df['Day_sin'] = df['Day'].apply(lambda x: math.sin(2*math.pi*x/31))\n",
    "    df['Day_cos'] = df['Day'].apply(lambda x: math.cos(2*math.pi*x/31))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd364c-8ca1-49d2-a51b-4f411d44859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_df = append_date_features(df=coin_df)\n",
    "coin_df = create_trigonometric_columns(df=coin_df)\n",
    "# Set date as index\n",
    "coin_df.set_index('Date', inplace=True)\n",
    "coin_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ee7b5-cbb0-4621-b207-42fab17d7deb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create the target variable\n",
    "\n",
    "The purpose of this function is to create a target variable that can be used for training a machine learning model to make predictions based on historical data. It achieves this by shifting the values of a specified column (commonly referred to as the \"Close\" column) in the DataFrame by a specified number of time steps (determined by the forecast_lead parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ad4622-668b-4c9d-9905-7e2f1f6bda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_variable(df: pd.DataFrame, forecast_lead: int = 1) -> (pd.DataFrame, str):    \n",
    "    target_column = \"Close\"\n",
    "    features = list(df.columns.difference([target_column]))\n",
    "    \n",
    "    target_name = f\"{target_column}_lead_{forecast_lead}\"\n",
    "    df[target_name] = df[target_column].shift(-forecast_lead)\n",
    "    df = df.iloc[:-forecast_lead]\n",
    "    return df, target_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98edd3c1-749a-41a3-b2bb-87cdaebb1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_df, target = create_target_variable(df=coin_df)\n",
    "display(\"Target added to dataframe\", coin_df[['Close', target]].head(), coin_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e6aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in coin_df.columns if col != target]\n",
    "features_str = ', '.join(features)\n",
    "display(Markdown(f\"<strong>Features:</strong> {features_str}<br><strong>Target:</strong> {target}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4104c3ca-b4e2-446e-b9a9-b8e1167de6bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split data\n",
    "* The purpose of the `split_train_valid_test` function is to split a pandas DataFrame into training, validation, and testing sets based on specific date ranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843a31e-0a82-4220-96e0-a22489297c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid_test(data: pd.DataFrame):    \n",
    "    # Split the data into training and testing sets\n",
    "    split_date_1 = datetime(2022, 1, 1)\n",
    "    split_date_2 = datetime(2022, 12, 1)\n",
    "    train_data = data.loc[data.index < split_date_1]\n",
    "    valid_data = data.loc[(split_date_1<= data.index) & (data.index <= split_date_2)]\n",
    "    test_data = data.loc[data.index > split_date_2]\n",
    "\n",
    "    return train_data, valid_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a116900",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = split_train_valid_test(data=coin_df)\n",
    "print(\"Train set fraction:\", round((len(train_data) / len(coin_df)), 2),'%')\n",
    "print(\"Valid set fraction:\", round((len(valid_data) / len(coin_df)), 2),'%')\n",
    "print(\"Test set fraction:\", round((len(test_data) / len(coin_df)), 2),'%')\n",
    "print(\"Train shape: \", train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb161f0-a368-401d-b95a-d1f7e7ddbb95",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c5dc1",
   "metadata": {},
   "source": [
    "This method creates a StandardScaler object and fits it on the training data only.\n",
    "The scaler is then applied to transform the training, validation, and test input feature data. Finally, the method concatenates the transformed input feature data with their respective target variable and returns the resulting scaled training, validation, and test data as pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80421c5a-b29f-448c-a5aa-ff3c850df308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scaling(train_data, valid_data, test_data, target):\n",
    "    # Separate the input features and target variable in each dataframe\n",
    "    X_train = train_data.drop(columns=[target])\n",
    "    y_train = train_data[target]\n",
    "\n",
    "    X_val = valid_data.drop(columns=[target])\n",
    "    y_val = valid_data[target]\n",
    "\n",
    "    X_test = test_data.drop(columns=[target])\n",
    "    y_test = test_data[target]\n",
    "\n",
    "    # Define a scaler object and fit it on the training data only\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    X_valid_scaled = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "    \n",
    "    train_scaled = pd.concat([X_train_scaled, y_train],axis = 1)\n",
    "    valid_scaled = pd.concat([X_valid_scaled, y_val],axis = 1)\n",
    "    test_scaled = pd.concat([X_test_scaled, y_test],axis = 1)\n",
    "    return train_scaled, valid_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a221db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled, valid_scaled, test_scaled = apply_scaling(train_data, valid_data, test_data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af21b743-1c20-4b6b-88f7-26a9a5d25b64",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f769624f",
   "metadata": {},
   "source": [
    "Class definition for a custom PyTorch dataset, SequenceDataset, which takes a pandas DataFrame and converts it into a PyTorch tensor for sequence modeling. It includes, \n",
    "- __init__() method to set up the dataset with the desired target variable, input features, and sequence length.\n",
    "- __len__() method returns the number of samples in the dataset\n",
    "- __getitem__() method returns a single sample as a tuple of the input sequence x and the corresponding target y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d162a3b-ad75-4a3a-a64c-f3e9fd09ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, target, features, sequence_length=5):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.y = torch.tensor(dataframe[target].values).float()\n",
    "        self.X = torch.tensor(dataframe[features].values).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i): \n",
    "        if i >= self.sequence_length - 1:\n",
    "            i_start = i - self.sequence_length + 1\n",
    "            x = self.X[i_start:(i + 1), :]\n",
    "        else:\n",
    "            padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n",
    "            x = self.X[0:(i + 1), :]\n",
    "            x = torch.cat((padding, x), 0)\n",
    "\n",
    "        return x, self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a65ab5-cd09-4f9c-bd92-075f3fc8ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "sequence_length = 3\n",
    "features = [col for col in train_scaled.columns if col != target]\n",
    "\n",
    "\n",
    "train_dataset = SequenceDataset(\n",
    "    train_scaled,\n",
    "    target=target,\n",
    "    features=features,\n",
    "    sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "X, y = train_dataset[i]\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419415eb-469b-4ddc-931a-e64adb7a34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[features].iloc[(i - sequence_length + 1): (i + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf7a4c-5e3d-4aa5-938b-2577d21c8700",
   "metadata": {},
   "source": [
    "### Creating and loading a PyTorch dataset and dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef2f07e",
   "metadata": {},
   "source": [
    "- The **get_dataset_obj** function takes in a Pandas dataframe dataframe, a list of features features, a target variable target, and a sequence length sequence_length. It creates a SequenceDataset object using these inputs and returns it.\n",
    "\n",
    "- The **get_dataloader** function takes in a dataset_obj, which is a SequenceDataset object, a batch size batch_size, and a boolean flag do_shuffle that indicates whether or not to shuffle the data. It creates a DataLoader object using these inputs and returns it. The DataLoader object is used to load the data in batches during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f7a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_obj(dataframe, features, target, sequence_length):\n",
    "    sequence_dataset = SequenceDataset(\n",
    "                            dataframe=dataframe,\n",
    "                            target=target,\n",
    "                            features=features,\n",
    "                            sequence_length=sequence_length\n",
    "                            )\n",
    "    return sequence_dataset\n",
    "    \n",
    "def get_dataloader(dataset_obj, batch_size, do_shuffle = False):\n",
    "    loader = DataLoader(dataset_obj, batch_size=batch_size, shuffle=do_shuffle)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f54ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(99)\n",
    "train_loader = DataLoader(train_dataset, batch_size=3)\n",
    "X, y = next(iter(train_loader))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67679b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 16\n",
    "train_dataset = get_dataset_obj(train_scaled, target=target, features=features, sequence_length=sequence_length)\n",
    "validation_dataset = get_dataset_obj(valid_scaled, target=target, features=features, sequence_length=sequence_length)\n",
    "test_dataset = get_dataset_obj(test_scaled, target=target, features=features, sequence_length=sequence_length)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = get_dataloader(train_dataset, batch_size=batch_size, do_shuffle=True)\n",
    "validation_loader = get_dataloader(validation_dataset, batch_size=batch_size)\n",
    "test_loader = get_dataloader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "X, y = next(iter(train_loader))\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306f1ad2",
   "metadata": {},
   "source": [
    "### Create DataLoaders for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900383ff",
   "metadata": {},
   "source": [
    "1. Load data of a crypto coin given its name and apply preprocessing.\n",
    "2. Create a target variable and split the data into training, validation, and testing sets.\n",
    "3. Apply scaling to the input features in each dataset.\n",
    "4. Initialize PyTorch Dataset objects for the scaled data with a specified sequence length and target variable.\n",
    "5. Initialize PyTorch DataLoader objects for each Dataset object with a specified batch size, and return the datasets, loaders, and target variable as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(coin: str, sequence_length, batch_size):\n",
    "    df = read_coin_data(coin_name=coin)\n",
    "    df.rename(columns={\"Time\":\"Date\"}, inplace=True)\n",
    "    df = append_date_features(df=df)\n",
    "    df = create_trigonometric_columns(df=df)\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df, target = create_target_variable(df=df)\n",
    "    train_df, valid_df, test_df = split_train_valid_test(data=df)\n",
    "    datasets = (train_df, valid_df, test_df)\n",
    "    train_scaled, valid_scaled, test_scaled = apply_scaling(train_df, valid_df, test_df, target)\n",
    "    features = [col for col in train_data.columns if col != target]\n",
    "    \n",
    "    # initialize Dataset objects\n",
    "    train_dataset = get_dataset_obj(train_scaled, target=target, features=features, sequence_length=sequence_length)\n",
    "    validation_dataset = get_dataset_obj(valid_scaled, target=target, features=features, sequence_length=sequence_length)\n",
    "    test_dataset = get_dataset_obj(test_scaled, target=target, features=features, sequence_length=sequence_length)\n",
    "\n",
    "    # initialize DataLoader objects\n",
    "    train_loader = get_dataloader(train_dataset, batch_size=batch_size)\n",
    "    validation_loader = get_dataloader(validation_dataset, batch_size=batch_size)\n",
    "    test_loader = get_dataloader(test_dataset, batch_size=batch_size)\n",
    "    loaders = (train_loader, validation_loader, test_loader)\n",
    "    \n",
    "    return datasets, loaders, target  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60741c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, loaders, target = prepare_data(coin='BTC', sequence_length=16, batch_size=16)\n",
    "train_loader, validation_loader, test_loader = loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3369ba-d6ff-4ebd-a192-6492e1057963",
   "metadata": {},
   "source": [
    "### LSTM architecture\n",
    "- Input:\n",
    "  - Number of features: `num_sensors`\n",
    "\n",
    "- LSTM Layer:\n",
    "  - Input size: `num_sensors`\n",
    "  - Hidden size: `hidden_units`\n",
    "  - Batch first: True\n",
    "  - Number of layers: `num_layers`\n",
    "\n",
    "- Fully Connected Layers:\n",
    "  - `fc1`:\n",
    "    - Input features: `hidden_units`\n",
    "    - Output features: 64\n",
    "  - Dropout (`dropout1`) with a probability of `dropout_prob`\n",
    "  - Batch normalization (`bn1`)\n",
    "  - ReLU activation function (`relu1`)\n",
    "  \n",
    "  - `fc2`:\n",
    "    - Input features: 64\n",
    "    - Output features: 16\n",
    "  - Dropout (`dropout2`) with a probability of `dropout_prob`\n",
    "  - Batch normalization (`bn2`)\n",
    "  - ReLU activation function (`relu2`)\n",
    "  \n",
    "  - `fc3`:\n",
    "    - Input features: 16\n",
    "    - Output features: 1\n",
    "\n",
    "- Forward Pass:\n",
    "  - Initialize the LSTM layer with zeros for the initial hidden state and cell state.\n",
    "  - Pass the input through the LSTM layer.\n",
    "  - Select the output of the last LSTM layer.\n",
    "  - Pass the output through `fc1`, `relu1`, `fc2`, `dropout2`, `relu2`, and `fc3`.\n",
    "  - Squeeze the output tensor to remove the last dimension of size 1.\n",
    "  - Return the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585235b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRegressionLSTM(nn.Module):\n",
    "    def __init__(self, num_sensors, hidden_units, num_layers, dropout_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.num_sensors = num_sensors  # this is the number of features\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_sensors,\n",
    "            hidden_size=hidden_units,  # Use the first value in the list for the first layer\n",
    "            batch_first=True,\n",
    "            num_layers=self.num_layers\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=hidden_units, out_features=64)\n",
    "        self.dropout1 = nn.Dropout(p=self.dropout_prob)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=16)\n",
    "        self.dropout2 = nn.Dropout(p=self.dropout_prob)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(in_features=16, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "\n",
    "        # Pass input through the first LSTM layer\n",
    "        out, (hn, _) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = hn[-1]  # Select the output of the last LSTM layer\n",
    "        out = self.fc1(out)\n",
    "#         out = self.dropout1(out)\n",
    "#         out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout2(out)\n",
    "#         out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out).squeeze()  # Squeeze to remove the last dimension of size 1\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a2877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_units = 64\n",
    "num_of_layers = 3\n",
    "model = DeepRegressionLSTM(num_sensors=15, hidden_units=num_hidden_units, num_layers=num_of_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65734be4",
   "metadata": {},
   "source": [
    "## Training details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93b9533",
   "metadata": {},
   "source": [
    "The **calculate_evalution_metrics** method calculates evaluation metrics for a given set of predictions and true values. The evaluation metrics include: \n",
    "- Mean Squared Error (MSE)\n",
    "- Mean Absolute Error (MAE)\n",
    "- R2 Score\n",
    "- Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4faa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_evaluation_metrics(y_pred, y_true, loss_fn):\n",
    "    mse = loss_fn(y_pred, y_true)\n",
    "    mae = torch.mean(torch.abs(y_pred - y_true))\n",
    "    r2 = torchmetrics.functional.r2_score(y_pred.view(-1), y_true.view(-1))\n",
    "    rmse = torch.sqrt(torch.mean(torch.pow(y_pred - y_true, 2)))\n",
    "    \n",
    "    return mse, mae, r2, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adfbedd",
   "metadata": {},
   "source": [
    "1. `plot_comparison(actual, pred, coin)`: \n",
    "   - Goal: Plot a comparison between the actual and predicted values for a given coin's close prices.\n",
    "   - Content: Plot the actual and predicted values on a graph.\n",
    "\n",
    "2. `train_model(data_loader, model, loss_function, optimizer, ix_epoch)`: \n",
    "   - Goal: Train a given model using the provided data loader, loss function, and optimizer.\n",
    "   - Content: \n",
    "     - Calculate evaluation metrics (MSE, MAE, R2, RMSE).\n",
    "     - Return the trained model and computed metrics.\n",
    "\n",
    "3. `evaluate_model(data_loader, model, loss_function, coin, ix_epoch = None)`: \n",
    "   - Goal: Evaluate a trained model using the provided data loader and loss function.\n",
    "   - Content: \n",
    "     - Calculate the loss and evaluation metrics (MSE, MAE, R2, RMSE).\n",
    "     - Plot a comparison between actual and predicted values if the epoch is divisible by 5.\n",
    "     - Return the evaluated metrics as a dictionary.\n",
    "\n",
    "4. `train_and_evaluate_model(train_loader, val_loader, model, loss_function, learning_rate, epochs, coin)`: \n",
    "   - Goal: Combine the training and evaluation processes for a given model.\n",
    "   - Content: \n",
    "     - Train the model using the provided training data loader, loss function, optimizer, learning rate, and epochs.\n",
    "     - Return the trained model.\n",
    "\n",
    "5. `predict(data_loader, model)`: \n",
    "   - Goal: Predict output values using a trained model.\n",
    "   - Content: Feed the input to the model and collect the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06acc926-a247-4c51-bba2-9ece385898b2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_comparison(actual, pred, coin):\n",
    "    plt.plot(actual, label='actual')\n",
    "    plt.plot(pred, label='prediction')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Close price')\n",
    "    plt.legend()\n",
    "    plt.title(f'{coin} Validation actual vs prediction')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_model(data_loader, model, loss_function, optimizer, ix_epoch) -> dict:\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    mse_list, mae_list, r2_list, rmse_list = [], [], [], []\n",
    "    \n",
    "    for X, y in data_loader:\n",
    "        output = model(X)\n",
    "        loss = loss_function(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # computes gradients of the loss\n",
    "        loss.backward()\n",
    "        # updates the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "        mse, mae, r2, rmse = calculate_evaluation_metrics(y_pred=output, y_true=y, loss_fn=loss_function)\n",
    "        mse_list.append(mse.item())\n",
    "        mae_list.append(mae.detach().numpy())\n",
    "        r2_list.append(r2.detach().numpy())\n",
    "        rmse_list.append(rmse.detach().numpy())\n",
    "    \n",
    "    mse = sum(mse_list) / num_batches\n",
    "    mae = sum(mae_list) / num_batches\n",
    "    r2 = sum(r2_list) / num_batches\n",
    "    rmse = sum(rmse_list) / num_batches\n",
    "    print(\"Epoch {}, Train || MSE: {:.7f}, MAE: {:.7f}, R2: {:.7f}, RMSE: {:.7f}\".format(ix_epoch, mse, mae, r2, rmse))\n",
    "    metrics = {'mse': mse, 'mae': mae, 'r2': r2, 'rmse': rmse}\n",
    "    return model, metrics\n",
    "\n",
    "def evaluate_model(data_loader, model, loss_function, coin, ix_epoch = None) -> dict:\n",
    "    \n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    mse_list, mae_list, r2_list, rmse_list = [], [], [], []\n",
    "    \n",
    "    model.eval()\n",
    "    actual_, pred_ = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            output = model(X)\n",
    "            total_loss += loss_function(output, y).item()\n",
    "            mse, mae, r2, rmse = calculate_evaluation_metrics(y_pred=output, y_true=y, loss_fn=loss_function)\n",
    "            mse_list.append(mse.item())\n",
    "            mae_list.append(mae.detach().numpy())\n",
    "            r2_list.append(r2.detach().numpy())\n",
    "            rmse_list.append(rmse.detach().numpy())\n",
    "            \n",
    "            actual_.append(y.numpy().reshape(-1))\n",
    "            pred_.append(output.numpy().reshape(-1))\n",
    "            \n",
    "        actual_ = np.hstack(actual_)\n",
    "        pred_ = np.hstack(pred_)\n",
    "        \n",
    "\n",
    "    mse = sum(mse_list) / num_batches\n",
    "    mae = sum(mae_list) / num_batches\n",
    "    r2 = sum(r2_list) / num_batches\n",
    "    rmse = sum(rmse_list) / num_batches\n",
    "    if ix_epoch is not None:\n",
    "        print(\"Epoch {}, Evaluation || MSE: {:.7f}, MAE: {:.7f}, R2: {:.7f}, RMSE: {:.7f}\".format(ix_epoch, mse, mae, r2, rmse))\n",
    "    metrics = {'mse': mse, 'mae': mae, 'r2': r2, 'rmse': rmse}\n",
    "    \n",
    "    # plot every 2 epochs\n",
    "    if ix_epoch is not None and ix_epoch % 5 == 0:   \n",
    "        plot_comparison(actual=actual_, pred=pred_, coin=coin)\n",
    "    return metrics\n",
    "\n",
    "def train_and_evaluate_model(train_loader, val_loader, model, loss_function, learning_rate, epochs, coin):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    start = time.time()\n",
    "    for ix_epoch in tqdm(range(epochs), desc=f\"Training {coin} coin...\"):\n",
    "        print(\"\\n---------\")\n",
    "        num_batches = len(train_loader)\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        mse_list, mae_list, r2_list, rmse_list = [], [], [], []\n",
    "\n",
    "        for X, y in train_loader:\n",
    "            output = model(X)\n",
    "            loss = loss_function(output, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # computes gradients of the loss\n",
    "            loss.backward()\n",
    "            # updates the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            mse, mae, r2, rmse = calculate_evaluation_metrics(y_pred=output, y_true=y, loss_fn=loss_function)\n",
    "            mse_list.append(mse.item())\n",
    "            mae_list.append(mae.detach().numpy())\n",
    "            r2_list.append(r2.detach().numpy())\n",
    "            rmse_list.append(rmse.detach().numpy())\n",
    "\n",
    "        mse = sum(mse_list) / num_batches\n",
    "        mae = sum(mae_list) / num_batches\n",
    "        r2 = sum(r2_list) / num_batches\n",
    "        rmse = sum(rmse_list) / num_batches\n",
    "        print(\"Epoch {}, Train || MSE: {:.7f}, MAE: {:.7f}, R2: {:.7f}, RMSE: {:.7f}\".format(ix_epoch+1, mse, mae, r2, rmse))\n",
    "        metrics = {'mse': mse, 'mae': mae, 'r2': r2, 'rmse': rmse}\n",
    "        val_metrics = evaluate_model(val_loader, model, loss_function, coin, ix_epoch=ix_epoch+1)\n",
    "        print()\n",
    "\n",
    "    return model\n",
    " \n",
    "    \n",
    "def predict(data_loader, model):\n",
    "\n",
    "    output = torch.tensor([])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, _ in data_loader:\n",
    "            y_pred = model(X)\n",
    "            output = torch.cat((output, y_pred), 0)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3fe224-4083-49c9-b32a-a48e95b51dce",
   "metadata": {},
   "source": [
    "### Train one model for each coin\n",
    "1. `train_all_coins(coin_list: list, epochs, learning_rate, loss_function, num_hidden_units, num_of_layers, batch_size, sequence_length)`:\n",
    "   - **Goal:** Train and evaluate regression models for a list of coins using the given parameters.\n",
    "   - **Content:**\n",
    "     - Initialize dictionaries and dataframes for storing model results, predictions, and actual values.\n",
    "     - Iterate over each coin in the coin list.\n",
    "     - Prepare data for the current coin.\n",
    "     - Create an instance of the `DeepRegressionLSTM` model.\n",
    "     - Train and evaluate the model using the training and validation data loaders.\n",
    "     - Evaluate the model on the test data and store the test metrics.\n",
    "     - Generate predictions using the trained model on the test data and store them in a dataframe.\n",
    "     - Store the actual values of the test data in a dataframe.\n",
    "     - Store the results, test predictions, and actual values for each coin.\n",
    "     - Return the model results, predictions dataframe, and actual values dataframe.\n",
    "\n",
    "2. `append_means(predictions_df, actual_df)`:\n",
    "   - **Goal:** Append the column-wise mean values to the predictions and actual dataframes.\n",
    "   - **Content:**\n",
    "     - Compute the mean values for each column in the predictions and actual dataframes.\n",
    "     - Append the mean values as new columns to the respective dataframes.\n",
    "     - Return the updated predictions and actual dataframes.\n",
    "\n",
    "3. `compute_mean_metrics(coin_results: dict)`:\n",
    "   - **Goal:** Compute the mean values of the test metrics for all the coins.\n",
    "   - **Content:**\n",
    "     - Initialize variables to store the sum of the test metrics and the total number of coins.\n",
    "     - Iterate over the results of each coin.\n",
    "     - Accumulate the test metric values.\n",
    "     - Calculate the mean values of the test metrics.\n",
    "     - Create a new dictionary with the mean results and return it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94597496-6b56-4c08-bbc4-a7d09713ce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_coins(coin_list: list, epochs, learning_rate, loss_function, num_hidden_units,\n",
    "                    num_of_layers, batch_size, sequence_length):\n",
    "    \n",
    "    \n",
    "    model_results = {\"learning_rate\": learning_rate, \"epochs\": epochs, \"batch_size\": batch_size}\n",
    "    model_results['results'] = {}\n",
    "    predictions_df = pd.DataFrame()\n",
    "    actual_df = pd.DataFrame()\n",
    "\n",
    "    for coin in tqdm(coin_list, desc=\"Processing coins...\"):\n",
    "        results = {}\n",
    "        datasets, loaders, target = prepare_data(coin=coin, sequence_length=sequence_length, batch_size=batch_size)\n",
    "        train_dataset, validation_dataset, test_dataset = datasets\n",
    "        train_loader, validation_loader, test_loader = loaders\n",
    "        \n",
    "        features = train_dataset.shape[1]-1\n",
    "        model = DeepRegressionLSTM(num_sensors=features, hidden_units=num_hidden_units, num_layers=num_of_layers)\n",
    "        \n",
    "        trained_model = train_and_evaluate_model(train_loader, validation_loader, model, loss_function,\n",
    "                                                  learning_rate, epochs, coin)\n",
    "        test_metrics = evaluate_model(test_loader, trained_model, loss_function, coin=coin, ix_epoch=None)\n",
    "        results['test_metrics'] = test_metrics\n",
    "        test_predictions = predict(test_loader, trained_model).numpy()\n",
    "        predictions_df[coin] = list(test_predictions)\n",
    "        actual_df[coin] = test_dataset[target].tolist()\n",
    "        model_results['results'][coin] = results\n",
    "        \n",
    "        \n",
    "    predictions_df.index = test_dataset.index\n",
    "    actual_df.index = test_dataset.index\n",
    "    return model_results, predictions_df, actual_df\n",
    "\n",
    "def append_means(predictions_df, actual_df):\n",
    "    predictions_df['mean'] = predictions_df.mean(axis=1)\n",
    "    actual_df['mean'] = actual_df.mean(axis=1)\n",
    "    return predictions_df, actual_df\n",
    "\n",
    "def compute_mean_metrics(coin_results: dict):\n",
    "    num_of_coins = len(coin_results)\n",
    "    sum_mse, sum_mae, sum_r2, sum_rmse, sum_time = 0, 0, 0, 0, 0\n",
    "    results_dict = coin_results['results']\n",
    "    for coin, results in results_dict.items():\n",
    "        sum_mse += results['test_metrics']['mse']\n",
    "        sum_mae += results['test_metrics']['mae']\n",
    "        sum_r2 += results['test_metrics']['r2']\n",
    "        sum_rmse += results['test_metrics']['rmse']\n",
    "\n",
    "    mean_results = coin_results    \n",
    "    mean_results['mean_mse'] = sum_mse/num_of_coins\n",
    "    mean_results['mean_mae'] = sum_mae/num_of_coins\n",
    "    mean_results['mean_r2'] = sum_r2/num_of_coins\n",
    "    mean_results['mean_rmse'] = sum_rmse/num_of_coins\n",
    "    \n",
    "    return mean_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d66c38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "loss_function = nn.MSELoss()\n",
    "# num_hidden_units = (64, 128)\n",
    "num_hidden_units = 128\n",
    "num_of_layers = 1\n",
    "batch_size = 16\n",
    "sequence_length = 16\n",
    "coin_results, predictions_df, actual_df = train_all_coins(available_coins, epochs, learning_rate, loss_function,\n",
    "                                                          num_hidden_units, num_of_layers, batch_size, sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66dcef0",
   "metadata": {},
   "source": [
    "### Save results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca975624",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "predictions_df, actual_df = append_means(predictions_df, actual_df)\n",
    "predictions_df.to_csv(f'./io/output/exports/predictions/LSTM_predictions_{timestamp}_epochs_{epochs}.csv')\n",
    "actual_df.to_csv(f'./io/output/exports/predictions/LSTM_actual_{timestamp}_epochs_{epochs}.csv')\n",
    "print('Dataframes saved!')\n",
    "\n",
    "mean_results = compute_mean_metrics(coin_results=coin_results)\n",
    "print(f\"mean mse: {round(mean_results['mean_mse'], 6)}\")\n",
    "print(f\"mean mae: {round(mean_results['mean_mae'], 6)}\")\n",
    "print(f\"mean r2: {round(mean_results['mean_r2'], 6)}\")\n",
    "print(f\"mean rmse: {round(mean_results['mean_rmse'], 6)}\")\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('./io/output/exports/metrics_plots/lstm_metrics.json', 'a') as f:\n",
    "    json.dump(mean_results, f, indent=len(mean_results))\n",
    "print(\"Results saved to json file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_actual_vs_pred(actual_df, pred_df, coin):\n",
    "#     plot_df = pd.DataFrame(index=pred_df.index)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    predictions = pred_df[coin]\n",
    "    actual_values = actual_df[coin]\n",
    "    plt.plot(pred_df.index, predictions, label='Predictions')\n",
    "    plt.plot(pred_df.index, actual_values, label='Actual')\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Close')\n",
    "    plt.title(f'{coin}: Closes vs Predictions')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.savefig(join(metrics_plot_path, f'LSTM_{coin}_pred_vs_true.pdf'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "coins_for_plot = ['BTC', 'ETH']\n",
    "for coin in coins_for_plot:\n",
    "    plot_test_actual_vs_pred(actual_df=actual_df, pred_df=predictions_df, coin=coin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b955210e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "full_ml",
   "language": "python",
   "name": "full_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
