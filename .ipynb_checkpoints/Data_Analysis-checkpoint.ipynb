{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b8f524",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cebc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99592a8",
   "metadata": {},
   "source": [
    "### Envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d48356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'io/input/data_raw/train.csv'\n",
    "assets_details_path ='io/input/data_raw/asset_details.csv'\n",
    "train_obj_path = 'io/input/data_intermidiate/train_object.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb7dc1",
   "metadata": {},
   "source": [
    "### Core Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de683a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbed8a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path, assets_details_path):\n",
    "    trainset = pd.read_csv(train_path,index_col=0)\n",
    "    assets_details = pd.read_csv(assets_details_path,index_col=0)\n",
    "    trainset.reset_index(inplace=True)\n",
    "    assets_details.reset_index(inplace=True)\n",
    "    trainset = pd.merge(trainset, assets_details[['Asset_ID', 'Weight', 'Asset_Name']],\n",
    "                         on='Asset_ID', how='inner', left_index=False)\n",
    "    trainset = reduce_mem_usage(trainset)\n",
    "    trainset['timestamp'] = pd.to_datetime(trainset['timestamp'], unit='s')\n",
    "    trainset = trainset.drop(['Asset_Name'], axis=1)\n",
    "    trainset['Target'].fillna(trainset['Target'].median(), inplace=True)\n",
    "    return trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e17dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273c907",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aa3529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_df(df):\n",
    "    print(f'Shape: {df.shape}')  \n",
    "    print(f'\\nColumn Names: {df.columns}')  \n",
    "    print(f'\\nData Types:')\n",
    "    print(df.dtypes) \n",
    "    print(f'\\nNon-Missing Values:')\n",
    "    print(df.count()) \n",
    "    print(f'\\nMissing Values:')\n",
    "    print(df.isnull().sum()) \n",
    "    print(f'\\nDistinct Values:')\n",
    "    print(df.nunique())  \n",
    "    print(f'\\nSummary Statistics:')\n",
    "    print(df.describe())\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f72ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_assets_ts(df):\n",
    "    df[\"Asset_Name\"] = df[\"Asset_Name\"].astype(\"category\")\n",
    "    grouped = df.groupby([\"Asset_Name\", \"timestamp\"]).sum()\n",
    "    sns.lineplot(x=\"timestamp\", y=\"Target\", hue=\"Asset_Name\", data=df)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    n = 90 # number of days between each tick\n",
    "    plt.title('Price by Asset')\n",
    "    plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=n))\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%Y'))\n",
    "    plt.savefig('Exports/Analysis_Plots/Target_by_Asset.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fce1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_assets_ts_weekly(df):\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(nrows=len(df['Asset_Name'].unique()), ncols=1, \n",
    "                             figsize=(10, 6*len(df['Asset_Name'].unique())))\n",
    "\n",
    "    # Group data by asset and resample on weekly basis\n",
    "    df_weekly = df.set_index('timestamp').groupby('Asset_Name').resample('W').sum().reset_index()\n",
    "    # Plot the time series for each asset name\n",
    "    for i, asset in enumerate(df['Asset_Name'].unique()):\n",
    "        ax = axes[i]\n",
    "        asset_df = df_weekly[df_weekly['Asset_Name'] == asset]\n",
    "        ax.plot(asset_df['timestamp'], asset_df['Target'])\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Target')\n",
    "        ax.set_title(asset)\n",
    "\n",
    "        # Format x-axis tick labels\n",
    "        ax.xaxis.set_major_locator(mdates.DayLocator(interval=200))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%Y'))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Exports/Analysis_Plots/Target_by_Asset_Weekly.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604381f",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_timestamps(df):\n",
    "    # Convert the 'timestamp' column to a pandas datetime object\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    # Get a list of all unique asset IDs in the dataset\n",
    "    asset_ids = df['Asset_ID'].unique()\n",
    "\n",
    "    # Get a list of all features in the dataset\n",
    "    features = list(df.columns)\n",
    "    features.remove('timestamp')\n",
    "    features.remove('Asset_ID')\n",
    "\n",
    "    # Group the original DataFrame by the timestamp column\n",
    "    grouped = df.groupby('timestamp')\n",
    "\n",
    "    # Create an empty list to store the resulting DataFrames\n",
    "    result = []\n",
    "\n",
    "    # Loop through each unique timestamp using tqdm\n",
    "    for name, group in tqdm(grouped):\n",
    "        group.set_index('Asset_ID', inplace=True)\n",
    "        group = group.sort_index(ascending=True)\n",
    "        result.append(group)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87207f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset=load_data(train_path, assets_details_path)\n",
    "trainset = split_timestamps(trainset)\n",
    "save_object(trainset, train_obj_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3530822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe_df(trainset)\n",
    "#null_rows = joined_df[joined_df['Target'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa1adf9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot_assets_ts_weekly(joined_df)\n",
    "#plot_assets_ts(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9255012",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1cde41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "full_ml",
   "language": "python",
   "name": "full_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
